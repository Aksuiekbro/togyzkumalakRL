{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AlphaZero for Togyz Kumalak — Kaggle Notebook\n",
        "\n",
        "This notebook sets up a lightweight, Kaggle-friendly pipeline for training an AlphaZero-style agent for Togyz Kumalak.\n",
        "\n",
        "- **Structure**: writes modules under `src/*` in the working directory and imports them.\n",
        "- **Persistence**: saves checkpoints and artifacts under `kaggle/working/alphazero_togyz/`.\n",
        "- **Config**: YAML config in `configs/default.yaml` controls self-play, model, MCTS, and training.\n",
        "- **Run flow**: Setup → Build modules → (optional) edit config → Train / Evaluate.\n",
        "\n",
        "You can run this end-to-end on a single Kaggle GPU session; it also supports checkpoint/resume between sessions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'IS_KAGGLE': False, 'WORK_DIR': '/Users/daurenzhunussov/togyzkumalak/alphazero_togyz', 'CHECKPOINT_DIR': '/Users/daurenzhunussov/togyzkumalak/alphazero_togyz/checkpoints'}\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'yaml'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myaml\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Non-interactive install\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yaml'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Non-interactive install\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip -q install --no-input pyyaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myaml\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yaml'"
          ]
        }
      ],
      "source": [
        "# Environment + minimal deps\n",
        "import os, sys, random, math\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect Kaggle\n",
        "IS_KAGGLE = os.path.exists(\"/kaggle\")\n",
        "WORK_DIR = Path(\"/kaggle/working/alphazero_togyz\") if IS_KAGGLE else Path.cwd() / \"alphazero_togyz\"\n",
        "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CHECKPOINT_DIR = WORK_DIR / \"checkpoints\"\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "ARTIFACTS_DIR = WORK_DIR / \"artifacts\"\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CONFIGS_DIR = WORK_DIR / \"configs\"\n",
        "CONFIGS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "SRC_DIR = WORK_DIR / \"src\"\n",
        "SRC_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print({\n",
        "    \"IS_KAGGLE\": IS_KAGGLE,\n",
        "    \"WORK_DIR\": str(WORK_DIR),\n",
        "    \"CHECKPOINT_DIR\": str(CHECKPOINT_DIR),\n",
        "})\n",
        "\n",
        "# Install minimal extras (most are preinstalled on Kaggle)\n",
        "try:\n",
        "    import yaml  # type: ignore\n",
        "except Exception:\n",
        "    # Non-interactive install\n",
        "    !pip -q install --no-input pyyaml\n",
        "    import yaml  # type: ignore\n",
        "\n",
        "try:\n",
        "    import tqdm  # type: ignore\n",
        "except Exception:\n",
        "    !pip -q install --no-input tqdm\n",
        "    import tqdm  # type: ignore\n",
        "\n",
        "# Basic reproducibility\n",
        "import numpy as np\n",
        "\n",
        "def set_global_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    try:\n",
        "        import torch\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "set_global_seed(42)\n",
        "print(\"Environment initialized.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sys.path[0] = /Users/daurenzhunussov/togyzkumalak/alphazero_togyz/src\n",
            "Scaffold ready.\n"
          ]
        }
      ],
      "source": [
        "# Project scaffolding and import path\n",
        "from pathlib import Path\n",
        "import sys, os\n",
        "\n",
        "# Create package structure\n",
        "for sub in [\n",
        "    SRC_DIR / \"game\",\n",
        "    SRC_DIR / \"encoding\",\n",
        "    SRC_DIR / \"mcts\",\n",
        "    SRC_DIR / \"nn\",\n",
        "    SRC_DIR / \"selfplay\",\n",
        "    SRC_DIR / \"trainer\",\n",
        "    SRC_DIR / \"arena\",\n",
        "    WORK_DIR / \"tests\",\n",
        "]:\n",
        "    sub.mkdir(parents=True, exist_ok=True)\n",
        "    (sub / \"__init__.py\").write_text(\"\")\n",
        "\n",
        "# Ensure import path\n",
        "if str(SRC_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_DIR))\n",
        "print(\"sys.path[0] =\", sys.path[0])\n",
        "\n",
        "# Utility: atomic write helper\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def atomic_write(path: Path, mode: str = \"w\"):\n",
        "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
        "    with open(tmp, mode) as f:\n",
        "        yield f\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "print(\"Scaffold ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate core module files under src/\n",
        "from textwrap import dedent\n",
        "\n",
        "files = {}\n",
        "\n",
        "files[SRC_DIR / \"nn\" / \"model.py\"] = dedent('''\n",
        "from typing import Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels: int, kernel_size: int = 3):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=padding, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(channels)\n",
        "        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=padding, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = F.relu(out + residual)\n",
        "        return out\n",
        "\n",
        "\n",
        "class AlphaZero1D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int = 8,\n",
        "        channels: int = 128,\n",
        "        num_blocks: int = 8,\n",
        "        board_len: int = 18,\n",
        "        num_actions: int = 9,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.board_len = board_len\n",
        "        padding = 1\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, channels, kernel_size=3, padding=padding, bias=False),\n",
        "            nn.BatchNorm1d(channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.blocks = nn.Sequential(*[ResidualBlock(channels, kernel_size=3) for _ in range(num_blocks)])\n",
        "        # Policy head\n",
        "        self.policy_conv = nn.Conv1d(channels, 2, kernel_size=3, padding=1, bias=False)\n",
        "        self.policy_bn = nn.BatchNorm1d(2)\n",
        "        self.policy_fc = nn.Linear(2 * board_len, num_actions)\n",
        "        # Value head\n",
        "        self.value_conv = nn.Conv1d(channels, channels, kernel_size=1, bias=False)\n",
        "        self.value_bn = nn.BatchNorm1d(channels)\n",
        "        self.value_fc1 = nn.Linear(channels, channels)\n",
        "        self.value_fc2 = nn.Linear(channels, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # x: (B, C, 18)\n",
        "        h = self.stem(x)\n",
        "        h = self.blocks(h)\n",
        "        # policy\n",
        "        p = F.relu(self.policy_bn(self.policy_conv(h)))\n",
        "        p = p.view(p.size(0), -1)\n",
        "        p_logits = self.policy_fc(p)\n",
        "        # value\n",
        "        v = F.relu(self.value_bn(self.value_conv(h)))\n",
        "        v = v.mean(dim=2)  # (B, channels)\n",
        "        v = F.relu(self.value_fc1(v))\n",
        "        v = torch.tanh(self.value_fc2(v)).squeeze(-1)\n",
        "        return p_logits, v\n",
        "''')\n",
        "\n",
        "files[SRC_DIR / \"game\" / \"togyzkumalak.py\"] = dedent('''\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "NUM_PITS_PER_SIDE = 9\n",
        "TOTAL_PITS = NUM_PITS_PER_SIDE * 2\n",
        "TOTAL_SEEDS = 162\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TogyzKumalakState:\n",
        "    pits: Tuple[int, ...]  # length 18\n",
        "    kazan: Tuple[int, int]\n",
        "    player: int  # 0 or 1; player to move\n",
        "    tuzdyk: Tuple[Optional[int], Optional[int]]  # per-side tuzdyk on opponent row (0..8)\n",
        "    move_count: int = 0\n",
        "\n",
        "    def legal_moves(self) -> List[int]:\n",
        "        start = 0 if self.player == 0 else NUM_PITS_PER_SIDE\n",
        "        end = start + NUM_PITS_PER_SIDE\n",
        "        return [i - start for i in range(start, end) if self.pits[i] > 0]\n",
        "\n",
        "    def is_terminal(self) -> bool:\n",
        "        if self.kazan[0] >= 82 or self.kazan[1] >= 82:\n",
        "            return True\n",
        "        start = 0 if self.player == 0 else NUM_PITS_PER_SIDE\n",
        "        end = start + NUM_PITS_PER_SIDE\n",
        "        if sum(self.pits[start:end]) == 0:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def winner(self) -> Optional[int]:\n",
        "        if not self.is_terminal():\n",
        "            return None\n",
        "        if self.kazan[0] >= 82:\n",
        "            return 0\n",
        "        if self.kazan[1] >= 82:\n",
        "            return 1\n",
        "        k0 = self.kazan[0] + sum(self.pits[0:NUM_PITS_PER_SIDE])\n",
        "        k1 = self.kazan[1] + sum(self.pits[NUM_PITS_PER_SIDE:TOTAL_PITS])\n",
        "        if k0 > k1:\n",
        "            return 0\n",
        "        if k1 > k0:\n",
        "            return 1\n",
        "        return None  # draw\n",
        "\n",
        "    def apply_move(self, action: int) -> \"TogyzKumalakState\":\n",
        "        raise NotImplementedError(\"Rules engine apply_move not yet implemented.\")\n",
        "\n",
        "    def to_fen(self) -> str:\n",
        "        pits_s = \",\".join(map(str, self.pits))\n",
        "        kaz_s = f\"{self.kazan[0]}|{self.kazan[1]}\"\n",
        "        t0 = -1 if self.tuzdyk[0] is None else self.tuzdyk[0]\n",
        "        t1 = -1 if self.tuzdyk[1] is None else self.tuzdyk[1]\n",
        "        return f\"{pits_s} {kaz_s} {self.player} {t0}:{t1} {self.move_count}\"\n",
        "\n",
        "\n",
        "def initial_state() -> TogyzKumalakState:\n",
        "    pits = tuple([9] * TOTAL_PITS)\n",
        "    return TogyzKumalakState(pits=pits, kazan=(0, 0), player=0, tuzdyk=(None, None), move_count=0)\n",
        "''')\n",
        "\n",
        "files[SRC_DIR / \"encoding\" / \"encoding.py\"] = dedent('''\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "from game.togyzkumalak import TogyzKumalakState, NUM_PITS_PER_SIDE, TOTAL_PITS, TOTAL_SEEDS\n",
        "\n",
        "\n",
        "def to_canonical(state: TogyzKumalakState, include_phase: bool = True) -> np.ndarray:\n",
        "    \"\"\"Encode state into (C, 18) canonical tensor.\n",
        "    Channels: [my_pits, opp_pits, my_tuz, opp_tuz, my_kazan, opp_kazan, move_phase]\n",
        "    \"\"\"\n",
        "    pits = np.array(state.pits, dtype=np.float32)\n",
        "    if state.player == 1:\n",
        "        # Flip perspective: bring current player to front\n",
        "        pits = np.concatenate([pits[NUM_PITS_PER_SIDE:], pits[:NUM_PITS_PER_SIDE]], axis=0)\n",
        "        t0, t1 = state.tuzdyk[1], state.tuzdyk[0]\n",
        "    else:\n",
        "        t0, t1 = state.tuzdyk\n",
        "\n",
        "    my = np.zeros(18, dtype=np.float32)\n",
        "    opp = np.zeros(18, dtype=np.float32)\n",
        "    my[:NUM_PITS_PER_SIDE] = pits[:NUM_PITS_PER_SIDE]\n",
        "    opp[NUM_PITS_PER_SIDE:] = pits[NUM_PITS_PER_SIDE:]\n",
        "\n",
        "    my_tuz = np.zeros(18, dtype=np.float32)\n",
        "    opp_tuz = np.zeros(18, dtype=np.float32)\n",
        "    if t0 is not None:\n",
        "        # my tuz lives on opponent row (right half in canonical)\n",
        "        opp_idx = NUM_PITS_PER_SIDE + t0\n",
        "        opp_tuz[opp_idx] = 1.0\n",
        "    if t1 is not None:\n",
        "        # opponent tuz on my row (left half)\n",
        "        my_idx = t1\n",
        "        my_tuz[my_idx] = 1.0\n",
        "\n",
        "    my_k = np.full(18, float(state.kazan[state.player] / TOTAL_SEEDS), dtype=np.float32)\n",
        "    opp_k = np.full(18, float(state.kazan[1 - state.player] / TOTAL_SEEDS), dtype=np.float32)\n",
        "\n",
        "    if include_phase:\n",
        "        # crude phase heuristic: normalize by 200 moves\n",
        "        phase = np.full(18, min(1.0, state.move_count / 200.0), dtype=np.float32)\n",
        "        feats = np.stack([my, opp, my_tuz, opp_tuz, my_k, opp_k, phase], axis=0)\n",
        "    else:\n",
        "        feats = np.stack([my, opp, my_tuz, opp_tuz, my_k, opp_k], axis=0)\n",
        "    return feats\n",
        "''')\n",
        "\n",
        "files[SRC_DIR / \"mcts\" / \"search.py\"] = dedent('''\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Optional, Tuple, List\n",
        "import math, random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class MCTSNode:\n",
        "    prior: float\n",
        "    visit_count: int = 0\n",
        "    total_value: float = 0.0\n",
        "    children: Dict[int, 'MCTSNode'] = field(default_factory=dict)\n",
        "\n",
        "    @property\n",
        "    def value(self) -> float:\n",
        "        if self.visit_count == 0:\n",
        "            return 0.0\n",
        "        return self.total_value / self.visit_count\n",
        "\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, model, encoder_fn, c_puct: float = 1.5, device: Optional[torch.device] = None):\n",
        "        self.model = model\n",
        "        self.encoder_fn = encoder_fn\n",
        "        self.c_puct = c_puct\n",
        "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _eval(self, state) -> Tuple[np.ndarray, float]:\n",
        "        x = self.encoder_fn(state)  # (C, 18)\n",
        "        xt = torch.from_numpy(x).float().unsqueeze(0).to(self.device)\n",
        "        logits, v = self.model(xt)\n",
        "        pi = torch.softmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
        "        return pi, float(v.item())\n",
        "\n",
        "    def search(self, root_state, num_simulations: int = 100, dirichlet_alpha: float = 0.3, dirichlet_eps: float = 0.25):\n",
        "        root = MCTSNode(prior=1.0)\n",
        "        priors, value = self._eval(root_state)\n",
        "        legal = root_state.legal_moves()\n",
        "        mask = np.zeros_like(priors)\n",
        "        for a in legal:\n",
        "            mask[a] = 1.0\n",
        "        priors = priors * mask\n",
        "        s = priors.sum()\n",
        "        if s > 0:\n",
        "            priors = priors / s\n",
        "        # Dirichlet noise at root\n",
        "        noise = np.random.dirichlet([dirichlet_alpha] * len(priors))\n",
        "        priors = (1 - dirichlet_eps) * priors + dirichlet_eps * noise\n",
        "        for a in range(len(priors)):\n",
        "            if priors[a] > 0:\n",
        "                root.children[a] = MCTSNode(prior=float(priors[a]))\n",
        "\n",
        "        for _ in range(num_simulations):\n",
        "            path: List[Tuple[MCTSNode, int]] = []\n",
        "            node = root\n",
        "            state = root_state\n",
        "            # Selection\n",
        "            while node.children:\n",
        "                best_score = -1e9\n",
        "                best_action = None\n",
        "                total_visits = sum(c.visit_count for c in node.children.values())\n",
        "                for a, child in node.children.items():\n",
        "                    u = self.c_puct * child.prior * math.sqrt(total_visits + 1e-8) / (1 + child.visit_count)\n",
        "                    score = child.value + u\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_action = a\n",
        "                path.append((node, best_action))\n",
        "                # Apply move\n",
        "                try:\n",
        "                    state = state.apply_move(best_action)\n",
        "                except NotImplementedError:\n",
        "                    break\n",
        "                node = node.children[best_action]\n",
        "\n",
        "            # Expansion\n",
        "            try:\n",
        "                priors, value = self._eval(state)\n",
        "            except Exception:\n",
        "                value = 0.0\n",
        "                priors = np.ones(9, dtype=np.float32) / 9.0\n",
        "            legal = state.legal_moves() if hasattr(state, 'legal_moves') else list(range(9))\n",
        "            mask = np.zeros_like(priors)\n",
        "            for a in legal:\n",
        "                mask[a] = 1.0\n",
        "            priors = priors * mask\n",
        "            s = priors.sum()\n",
        "            if s > 0:\n",
        "                priors = priors / s\n",
        "            leaf = MCTSNode(prior=1.0)\n",
        "            for a in range(len(priors)):\n",
        "                if priors[a] > 0:\n",
        "                    leaf.children[a] = MCTSNode(prior=float(priors[a]))\n",
        "\n",
        "            # Backprop\n",
        "            for parent, action in reversed(path):\n",
        "                child = parent.children[action]\n",
        "                child.visit_count += 1\n",
        "                child.total_value += value\n",
        "\n",
        "        visit_counts = np.zeros(9, dtype=np.float32)\n",
        "        for a, child in root.children.items():\n",
        "            visit_counts[a] = child.visit_count\n",
        "        if visit_counts.sum() > 0:\n",
        "            policy = visit_counts / visit_counts.sum()\n",
        "        else:\n",
        "            policy = np.ones(9, dtype=np.float32) / 9.0\n",
        "        return policy\n",
        "''')\n",
        "\n",
        "files[SRC_DIR / \"selfplay\" / \"worker.py\"] = dedent('''\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "\n",
        "def generate_self_play_games(*args, **kwargs):\n",
        "    \"\"\"Placeholder for self-play episodes generator.\n",
        "    Returns a list of (state_tensor, policy_target, value_target) triplets.\n",
        "    \"\"\"\n",
        "    return []\n",
        "''')\n",
        "\n",
        "files[SRC_DIR / \"trainer\" / \"loop.py\"] = dedent('''\n",
        "from typing import Dict, Iterable\n",
        "\n",
        "\n",
        "def train_loop(*args, **kwargs):\n",
        "    \"\"\"Placeholder training loop with AMP, clipping, and checkpointing.\"\"\"\n",
        "    pass\n",
        "''')\n",
        "\n",
        "files[SRC_DIR / \"arena\" / \"eval.py\"] = dedent('''\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "def evaluate_gating(*args, **kwargs) -> float:\n",
        "    \"\"\"Placeholder arena evaluation; returns win-rate vs. baseline.\"\"\"\n",
        "    return 0.5\n",
        "''')\n",
        "\n",
        "files[SRC_DIR / \"agent.py\"] = dedent('''\n",
        "from typing import Optional\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def select_move(policy: np.ndarray, valid_mask: Optional[np.ndarray] = None) -> int:\n",
        "    \"\"\"Select argmax over masked policy.\"\"\"\n",
        "    p = policy.copy()\n",
        "    if valid_mask is not None:\n",
        "        p *= valid_mask\n",
        "        s = p.sum()\n",
        "        if s > 0:\n",
        "            p /= s\n",
        "    return int(np.argmax(p))\n",
        "''')\n",
        "\n",
        "for path, content in files.items():\n",
        "    with atomic_write(path) as f:\n",
        "        f.write(content)\n",
        "    print(\"Wrote\", path.relative_to(WORK_DIR))\n",
        "\n",
        "print(\"Core modules generated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write default config to configs/default.yaml\n",
        "import yaml\n",
        "from pprint import pprint\n",
        "\n",
        "config = {\n",
        "    \"model\": {\n",
        "        \"in_channels\": 8,\n",
        "        \"channels\": 128,\n",
        "        \"num_blocks\": 8,\n",
        "    },\n",
        "    \"mcts\": {\n",
        "        \"c_puct\": 1.5,\n",
        "        \"dirichlet_alpha\": 0.3,\n",
        "        \"dirichlet_eps\": 0.25,\n",
        "        \"simulations\": 160,\n",
        "    },\n",
        "    \"selfplay\": {\n",
        "        \"temperature_moves\": 10,\n",
        "        \"num_games_per_iter\": 256,\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"buffer_size\": int(1e6),\n",
        "        \"batch_size\": 512,\n",
        "        \"lr\": 1e-3,\n",
        "        \"weight_decay\": 1e-4,\n",
        "    },\n",
        "    \"arena\": {\n",
        "        \"num_games\": 200,\n",
        "        \"gate_win_rate\": 0.55,\n",
        "        \"confidence\": 0.95,\n",
        "    },\n",
        "    \"paths\": {\n",
        "        \"work_dir\": str(WORK_DIR),\n",
        "        \"checkpoints\": str(CHECKPOINT_DIR),\n",
        "        \"artifacts\": str(ARTIFACTS_DIR),\n",
        "    },\n",
        "}\n",
        "\n",
        "CONFIGS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "with open(CONFIGS_DIR / \"default.yaml\", \"w\") as f:\n",
        "    yaml.safe_dump(config, f, sort_keys=False)\n",
        "\n",
        "print(\"Wrote\", (CONFIGS_DIR / \"default.yaml\").relative_to(WORK_DIR))\n",
        "pprint(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smoke test: import model and run dummy forward\n",
        "import torch\n",
        "from nn.model import AlphaZero1D\n",
        "\n",
        "model = AlphaZero1D(in_channels=8, channels=128, num_blocks=4)\n",
        "model.eval()\n",
        "\n",
        "x = torch.randn(2, 8, 18)\n",
        "with torch.no_grad():\n",
        "    logits, v = model(x)\n",
        "print(\"policy logits:\", logits.shape, \"value:\", v.shape)\n",
        "assert logits.shape == (2, 9) and v.shape == (2,), \"Unexpected output shapes\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to use on Kaggle\n",
        "\n",
        "1. Run the setup cells above (Environment, Scaffolding, Core modules, Config).\n",
        "2. Implement game rules in `src/game/togyzkumalak.py:apply_move` (and add unit tests under `tests/`).\n",
        "3. Implement self-play episodes in `src/selfplay/worker.py` and the training loop in `src/trainer/loop.py`.\n",
        "4. Wire up MCTS from `src/mcts/search.py` with the model from `src/nn/model.py` and the encoder `src/encoding/encoding.py`.\n",
        "5. Start a training iteration: generate self-play, train, evaluate arena, then checkpoint under `checkpoints/`.\n",
        "6. To resume next session, ensure you reload the latest checkpoint from `checkpoints/`.\n",
        "\n",
        "Notes:\n",
        "- All files are written under `alphazero_togyz/` in `kaggle/working`.\n",
        "- Tweak `configs/default.yaml` to change MCTS sims, network size, batch size, etc.\n",
        "- Optional logging tools (e.g., Weights & Biases) can be added later.\n",
        "\n",
        "Next steps (from the plan):\n",
        "- Rules engine with tuzdyk and terminal states.\n",
        "- Canonical state encoding, symmetry, and hashing.\n",
        "- PUCT MCTS with transposition table and Dirichlet noise.\n",
        "- 1D residual PyTorch model (already in place) with policy/value heads.\n",
        "- Self-play worker with temperature schedule and batching.\n",
        "- Replay buffer, AMP training, checkpoint/resume.\n",
        "- Evaluation arena and Elo with gating.\n",
        "- Kaggle GPU training loop with resume and logging.\n",
        "- CLI agent running MCTS on a given position.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alpha–Beta Engine (No Training) — Quick Start\n",
        "\n",
        "This section adds a strong, training-free engine using iterative-deepening alpha–beta with a simple heuristic. It plays well immediately and fits within 1 hour on Kaggle. You can tweak hyperparameters (time per move, max depth, evaluation weights) below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement full rules: overwrite src/game/togyzkumalak.py apply_move\n",
        "from pathlib import Path\n",
        "from textwrap import dedent\n",
        "\n",
        "rules_impl = dedent('''\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "NUM_PITS_PER_SIDE = 9\n",
        "TOTAL_PITS = NUM_PITS_PER_SIDE * 2\n",
        "TOTAL_SEEDS = 162\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TogyzKumalakState:\n",
        "    pits: Tuple[int, ...]  # length 18\n",
        "    kazan: Tuple[int, int]\n",
        "    player: int  # 0 or 1; player to move\n",
        "    tuzdyk: Tuple[Optional[int], Optional[int]]  # per-side tuzdyk on opponent row (0..8)\n",
        "    move_count: int = 0\n",
        "\n",
        "    def legal_moves(self) -> List[int]:\n",
        "        start = 0 if self.player == 0 else NUM_PITS_PER_SIDE\n",
        "        end = start + NUM_PITS_PER_SIDE\n",
        "        return [i - start for i in range(start, end) if self.pits[i] > 0]\n",
        "\n",
        "    def is_terminal(self) -> bool:\n",
        "        if self.kazan[0] >= 82 or self.kazan[1] >= 82:\n",
        "            return True\n",
        "        start = 0 if self.player == 0 else NUM_PITS_PER_SIDE\n",
        "        end = start + NUM_PITS_PER_SIDE\n",
        "        if sum(self.pits[start:end]) == 0:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def winner(self) -> Optional[int]:\n",
        "        if not self.is_terminal():\n",
        "            return None\n",
        "        if self.kazan[0] >= 82:\n",
        "            return 0\n",
        "        if self.kazan[1] >= 82:\n",
        "            return 1\n",
        "        k0 = self.kazan[0] + sum(self.pits[0:NUM_PITS_PER_SIDE])\n",
        "        k1 = self.kazan[1] + sum(self.pits[NUM_PITS_PER_SIDE:TOTAL_PITS])\n",
        "        if k0 > k1:\n",
        "            return 0\n",
        "        if k1 > k0:\n",
        "            return 1\n",
        "        return None  # draw\n",
        "\n",
        "    def apply_move(self, action: int) -> \"TogyzKumalakState\":\n",
        "        pits = list(self.pits)\n",
        "        k0, k1 = self.kazan\n",
        "        p = self.player\n",
        "        t0, t1 = self.tuzdyk\n",
        "\n",
        "        start = 0 if p == 0 else 9\n",
        "        src = start + action\n",
        "        stones = pits[src]\n",
        "        if stones <= 0:\n",
        "            raise ValueError(\"Illegal move from empty pit\")\n",
        "        pits[src] = 0\n",
        "\n",
        "        # Absolute tuzdyk indices\n",
        "        tuz_abs0 = (9 + t0) if t0 is not None else None  # P0's tuz on P1 row\n",
        "        tuz_abs1 = (0 + t1) if t1 is not None else None  # P1's tuz on P0 row\n",
        "\n",
        "        last_idx: Optional[int] = None\n",
        "        pos = src\n",
        "        if stones > 1:\n",
        "            # Drop first stone into starting pit\n",
        "            pits[pos] += 1\n",
        "            remaining = stones - 1\n",
        "            while remaining > 0:\n",
        "                pos = (pos + 1) % 18\n",
        "                if tuz_abs0 is not None and pos == tuz_abs0:\n",
        "                    k0 += 1\n",
        "                elif tuz_abs1 is not None and pos == tuz_abs1:\n",
        "                    k1 += 1\n",
        "                else:\n",
        "                    pits[pos] += 1\n",
        "                    last_idx = pos\n",
        "                remaining -= 1\n",
        "        else:\n",
        "            # single stone → place in next pit\n",
        "            pos = (src + 1) % 18\n",
        "            if tuz_abs0 is not None and pos == tuz_abs0:\n",
        "                k0 += 1\n",
        "                last_idx = None\n",
        "            elif tuz_abs1 is not None and pos == tuz_abs1:\n",
        "                k1 += 1\n",
        "                last_idx = None\n",
        "            else:\n",
        "                pits[pos] += 1\n",
        "                last_idx = pos\n",
        "\n",
        "        new_t0, new_t1 = t0, t1\n",
        "\n",
        "        # Captures / tuzdyk creation only if last stone ended in opponent pit\n",
        "        if last_idx is not None:\n",
        "            if p == 0 and 9 <= last_idx <= 17:\n",
        "                local = last_idx - 9\n",
        "                cnt = pits[last_idx]\n",
        "                if cnt == 3 and new_t0 is None and local != 8 and (t1 is None or local != t1):\n",
        "                    k0 += 3\n",
        "                    pits[last_idx] = 0\n",
        "                    new_t0 = local\n",
        "                elif cnt % 2 == 0 and cnt > 0:\n",
        "                    k0 += cnt\n",
        "                    pits[last_idx] = 0\n",
        "            elif p == 1 and 0 <= last_idx <= 8:\n",
        "                local = last_idx - 0\n",
        "                cnt = pits[last_idx]\n",
        "                if cnt == 3 and new_t1 is None and local != 8 and (t0 is None or local != t0):\n",
        "                    k1 += 3\n",
        "                    pits[last_idx] = 0\n",
        "                    new_t1 = local\n",
        "                elif cnt % 2 == 0 and cnt > 0:\n",
        "                    k1 += cnt\n",
        "                    pits[last_idx] = 0\n",
        "\n",
        "        return TogyzKumalakState(\n",
        "            pits=tuple(pits),\n",
        "            kazan=(k0, k1),\n",
        "            player=1 - p,\n",
        "            tuzdyk=(new_t0, new_t1),\n",
        "            move_count=self.move_count + 1,\n",
        "        )\n",
        "''')\n",
        "\n",
        "with atomic_write(SRC_DIR / \"game\" / \"togyzkumalak.py\") as f:\n",
        "    f.write(rules_impl)\n",
        "print(\"Updated src/game/togyzkumalak.py with full apply_move.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alpha–Beta engine (negamax + alpha-beta + transposition table)\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "import time\n",
        "\n",
        "INF = 10_000_000\n",
        "\n",
        "EVAL_WEIGHTS = {\n",
        "    \"kazan\": 1000,   # weight for kazan difference\n",
        "    \"row\": 5,        # stones on my row vs opponent row\n",
        "    \"tuz\": 200,      # having a tuzdyk\n",
        "    \"pits\": 2,       # non-empty pits difference\n",
        "}\n",
        "\n",
        "\n",
        "def eval_state(state) -> int:\n",
        "    me = state.player\n",
        "    opp = 1 - me\n",
        "    pits = state.pits\n",
        "    my_row = pits[0:9] if me == 0 else pits[9:18]\n",
        "    opp_row = pits[9:18] if me == 0 else pits[0:9]\n",
        "    my_k, opp_k = state.kazan[me], state.kazan[opp]\n",
        "    my_tuz = 1 if state.tuzdyk[me] is not None else 0\n",
        "    opp_tuz = 1 if state.tuzdyk[opp] is not None else 0\n",
        "\n",
        "    s = 0\n",
        "    s += EVAL_WEIGHTS[\"kazan\"] * (my_k - opp_k)\n",
        "    s += EVAL_WEIGHTS[\"row\"]   * ((sum(my_row)) - (sum(opp_row)))\n",
        "    s += EVAL_WEIGHTS[\"tuz\"]   * (my_tuz - opp_tuz)\n",
        "    s += EVAL_WEIGHTS[\"pits\"]  * ((sum(1 for x in my_row if x > 0)) - (sum(1 for x in opp_row if x > 0)))\n",
        "    return s\n",
        "\n",
        "\n",
        "def order_moves(state, moves: List[int]) -> List[int]:\n",
        "    start = 0 if state.player == 0 else 9\n",
        "    return sorted(moves, key=lambda a: state.pits[start + a], reverse=True)\n",
        "\n",
        "\n",
        "def negamax(state, depth: int, alpha: int, beta: int, tt: Dict[str, Tuple[int,int]], deadline: float) -> Tuple[int, Optional[int]]:\n",
        "    if time.time() >= deadline:\n",
        "        return eval_state(state), None\n",
        "    key = state.to_fen()\n",
        "    if key in tt:\n",
        "        d_cached, score_cached = tt[key]\n",
        "        if d_cached >= depth:\n",
        "            return score_cached, None\n",
        "    if state.is_terminal():\n",
        "        w = state.winner()\n",
        "        if w is None:\n",
        "            return 0, None\n",
        "        return (INF - 1 if w == state.player else -(INF - 1)), None\n",
        "    if depth == 0:\n",
        "        return eval_state(state), None\n",
        "\n",
        "    best_move = None\n",
        "    value = -INF\n",
        "    for a in order_moves(state, state.legal_moves()):\n",
        "        child = state.apply_move(a)\n",
        "        sc, _ = negamax(child, depth - 1, -beta, -alpha, tt, deadline)\n",
        "        sc = -sc\n",
        "        if sc > value:\n",
        "            value = sc\n",
        "            best_move = a\n",
        "        if value > alpha:\n",
        "            alpha = value\n",
        "        if alpha >= beta or time.time() >= deadline:\n",
        "            break\n",
        "    tt[key] = (depth, value)\n",
        "    return value, best_move\n",
        "\n",
        "\n",
        "class ABEngine:\n",
        "    def __init__(self):\n",
        "        self.tt: Dict[str, Tuple[int,int]] = {}\n",
        "\n",
        "    def choose(self, state, max_time_ms: int = 600, max_depth: int = 8) -> int:\n",
        "        deadline = time.time() + (max_time_ms / 1000.0)\n",
        "        last_best = 0\n",
        "        for d in range(2, max_depth + 1):\n",
        "            sc, bm = negamax(state, d, -INF, INF, self.tt, deadline)\n",
        "            if bm is not None:\n",
        "                last_best = bm\n",
        "            if time.time() >= deadline:\n",
        "                break\n",
        "        return last_best\n",
        "\n",
        "print(\"Alpha–Beta engine ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters + example usage\n",
        "from game.togyzkumalak import initial_state\n",
        "\n",
        "# Tunable knobs\n",
        "MAX_TIME_MS_OPENING = 400   # 300–600\n",
        "MAX_TIME_MS_ENDGAME = 900   # 800–1200\n",
        "MAX_DEPTH = 8               # 6–8 typical in Python for ~0.6s\n",
        "\n",
        "# Evaluation weights (increase to bias behavior)\n",
        "EVAL_WEIGHTS[\"kazan\"] = 1000\n",
        "EVAL_WEIGHTS[\"tuz\"]   = 200\n",
        "EVAL_WEIGHTS[\"row\"]   = 5\n",
        "EVAL_WEIGHTS[\"pits\"]  = 2\n",
        "\n",
        "engine = ABEngine()\n",
        "state = initial_state()\n",
        "\n",
        "# Small demo: play 10 plies with the engine moving for both sides\n",
        "for ply in range(10):\n",
        "    budget = MAX_TIME_MS_OPENING if ply < 30 else MAX_TIME_MS_ENDGAME\n",
        "    mv = engine.choose(state, max_time_ms=budget, max_depth=MAX_DEPTH)\n",
        "    state = state.apply_move(mv)\n",
        "print(\"Demo plies executed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick timing smoke test\n",
        "import time\n",
        "from game.togyzkumalak import initial_state\n",
        "\n",
        "s = initial_state()\n",
        "engine = ABEngine()\n",
        "start = time.time()\n",
        "plies = 20\n",
        "for ply in range(plies):\n",
        "    mv = engine.choose(s, max_time_ms=300, max_depth=8)\n",
        "    s = s.apply_move(mv)\n",
        "elapsed = time.time() - start\n",
        "print({\"plies\": plies, \"elapsed_sec\": round(elapsed, 2), \"per_ply_ms\": int(1000*elapsed/plies)})\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
